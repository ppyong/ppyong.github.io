---
layout: post
title: 왜 MongoDB인가? 
---

해당 내용은 Real MongoDB책을 읽고 정리한 내용입니다. 추가로 책을 읽으면서 궁금한 것들도 추후 조사하기 위해 남겨뒀습니다 :) 

먼저, MongoDB가 가진 특징들을 나열하고 상세한 설명을 이어가는 식으로 작성하였습니다. 

MongoDB의 특징은 무엇이 있을까? 
> - 트랜잭션 지원
> - 분산 처리
> - 재해 복구
> - Sharding & Re-Balancing
> - 데이터 복제 & 자동 복구

&nbsp;
## 트랜잭션 지원
>MongoDB의 트랜잭션은 흔히 생각하는 MySql과 같은 RDBM에서 말하는 트랜잭션과는 조금 다른 의미를 가진다. 
흔히 트랜잭션을 언급할 때 Multi Query (DML)에 대해서 ACID를 말하곤 하는데, MongoDB에서는 이런 Multi Query에 대해서 트랜잭션을 지원하지는 않는다. 정확히는 이 책은 주로 MongoDB v3.2 ~ v3.6을 기준으로 설명하고 있기에 해당 버전에서는 지원하지 않는다는 의미이다. v4.0 이상부터는 지원하는 것으로 알고 있지만 추가로 학습이 필요한 부분이다. 
>
>MySql과 같은 DBMS에서는 아래와 같이 Multi Query를 한 트랙잭션 단위로 묶어서 처리가 가능하다. 

```sql 
START TRANSACTION; 
INSERT INTO ${TABLE_NAME} VALUES ('${ID}', '${TITLE}', '${CONTENT}'); --(1)
INSERT INTO ${TABLE_NAME} VALUES ('${ID}', '${TITLE}', '${CONTENT}'); --(2)
COMMIT;
```

>MongoDB에서 트랜잭션을 지원한다는 의미는 DML을 통해 데이터를 변경 시 OpLog 컬렉션에도 변경 기록을 남기게 되는데 이 컬렉션과 트랜잭션으로 묶여서 처리된다를 의미한다. 

&nbsp;
## 분산처리
>MongoDB는 별도의 서드 파티 도구를 사용하지 않아도 자체적으로 Sharding을 지원하고 있다. 그로 인해 어플리케이션이 거대해지고 사용자 요청이 많아짐에 따라 Write 에 대해 병목이 발생할 수 있는데, 이런 부분을 Sharding을 통해 해소해줄 수 있다. 

&nbsp;
## 재해 복구
>MySql과 같은 데이터베이스에서는 HA를 구현하기 위해서는 MHA나 MMM과 같은 별도의 솔루션을 사용하여야 하지만 MongoDB의 경우 자체적으로 Primary에 문제가 생겼을 시 Secondary를 자동으로 승급시켜 Primary 장애시에도 서비스를 유지할 수 있게 해준다. 

&nbsp;
## Sharding & Re-Balancing 
>MySql과 같은 RDBM에서 Sharding을 구현하기 위해서는 어플리케이션 단에서 처리하거나 해당 링크(https://www.youtube.com/watch?v=8Eb_n7JA1yA)에서 설명하는거와 같이 별도의 오픈소스 솔루션을 선택해야 했는데, MongoDB는 별도의 솔루션 도입 없이 Sharding 기능을 내장하고 있으며 특정 Shard에 데이터가 몰려 부하가 집중되지 않도록 Chunk 스플릿을 통해 Chunk를 쪼개고 여러 Shard로 분산하는 Re-Balancing 기능을 자체적으로 지원하고 있다. 

## 데이터 복제 & 자동 복구
>Replication 기능을 제공하여 Primary에서 변경된 데이터에 대해 Secondary가 데이터를 복제하고 Primary 비정상 종료 혹은 네트워크 장애 발생 시 Secondary를 투표를 통해 승급 처리하여 Primary로 활용함으로서 서비스를 안정적으로 유지할 수 있다.

&nbsp;
&nbsp;

아래부터는 위 내용에 대한 설명을 위해 MongoDB의 기본적인 아키텍쳐 및 기능들에 대한 설명을 이어나갈 것이다. 

_MongoDB 와 RDBMS (MySQL)이 객체 이름이 조금 다르기에 아래와 같이 표를 작성하였다._

| MongoDB| MySQL |
| ------ | ------ |
| 데이터베이스 | 데이터베이스 |
| 컬렉션 | 테이블 |
| 도큐먼트 | 레코드 |
| 필드 | 컬럼 |
| 인덱스 | 인덱스 |
| 쿼리의 결과로 "커서" 반환 | 쿼리의 결과로 "레코드" 반환 |

&nbsp;
&nbsp;
# 1. 아키텍쳐 



# 2. 스토리지 
> **WiredTiger의 내부 동작 방식** 

- 다른 DBMS와 동일하게 B-Tree 구조의 데이터 파일과 서버 크래시로부터 데이터를 복구하기 위한 저널로그를 가지고 있다. 저널로그는 RDBM 의 리두 로그처럼 로테이션 되는 방식이 아닌 새로운 로그 파일이 계속 생산되고 체크포인트 시점 이전의 저널 로그는 삭제한다.
 
     → 공유 캐시가 어느정도 쌓이면 WiredTiger 스토리지 엔진은 체크포인트를 발생시킨다 
 
- 캐시 이빅션 모듈은 공유 캐시가 적절한 메모리 사용량을 유지하도록 공유 캐시에서 자주 사용되지 않는 데이터페이지를 제거하는 작업을 수행한다. 만약 제거해야하는 데이터가 더티 페이지라면 리컨실리에이션 모듈을 이용해 디스크에 데이터를 기록하고 공유 캐시에서 제거한다
 
     → 이때 트랜젝션 로그는 어떻게 되는거지? 더티 페이지가 반영되었다는 의미는 해당 트랜잭션 로그 중 일부가 반영되었다는 의미 아닌가? (**Need to research**)
 
> **공유캐시** 

- MongoDB 3.4부터 기본적으로 전체 메모리의 50% ~ 1G로 설정된다. 만약 이 값이 256M보다 적을 때는 256으로 설정된다.
 
- 서버를 재시작 하지 않고도 크기 조정 가능하다. 단, 내부 동시 처리 작업들을 멈추고 크기를 변경하게 되므로 쿼리 처리량이나 부하가 높을 때에는 주의가 필요하다.
 
> **하자드 포인터**

- WiredTiger 스토리지 엔진은 모든 사용자 Thread는 캐시 데이터를 참조할 때 하자드 포인터에 자신이 참조하는 페이지를 등록한다. 이빅션 Thread는 제거 해도 될 페이지를 골라 삭제 전에 하자드 포인터에 등록되어 있는지 확인 후 등록되어 있지 않을 경우에만 제거한다. 이런 방식으로 잠금 대기 없이 임무를 수행할 수 있다. 
 
> **스킵 리스트**
 
> **체크 포인트**

- 사용자의 요청을 빠르게 처리하면서 커밋된 트랜젝션의 영속성을 보장하기 위해서 트랜젝셔 로그 (WAL, 저널로그)를 먼저 기록하고 데이터 파일에 기록하는 작업은 사용자의 트랜젝션과 관련없이 뒤로 미뤄서 처리한다. 이런 트랜젝션 DBMS는 모두 체크포인트라는 개념을 가지고 있는데, 데이터파일과 트랜젝션 로그가 동기화 되는 시점을 의미한다.
 
- 체크포인트는 DBMS 서버가 크래시되거나 응답 불능으로 인해서 비정상 종료됐다가 재시작됐을 때 복구를 시작할 시점을 결정하는 기준이 된다. 그래서 체크포인트 간격이 너무 길면 DBMS 복구 시간이 길어지게 되고, 너무 빈번하면 서버가 쿼리를 처리하는 능력이 떨어진다
 
- WiredTiger 스토리지 엔진은 샤프 체크포인트 방식을 채택하고 있다.

    - _~~여기서 퍼지 체크포인트는 조금 오래전 시점에 발생했던 트랙잭션을 체크포인트 기준점으로 선택한다 하는데 이게 어떤 의미일까?~~_ (**Need to research**)
    
    - WiredTiger 스토리지 엔진은 평상시에는 쓰기를 거의 하지 않고 체크포인트가 발생하는 시점에 상당히 많은 쓰기가 순간적으로 발생한다.
    
    - 레플리카셋의 세컨더리에서 저널로그를 활성화하지 않은 경우 데이터의 영구적인 보관을 위해 변경된 데이터를 항상 파일에 동기화해야한다. 이를 위해 MongoDB 3.2부터는 레플리카셋에 투입된 세컨더리 맴버가 저널로그가 없을 경우 계속해서 체크포인트를 실행한다.  (p.88 중간)
 
         → ~~_체크포인트가 트랜잭션 로그와 데이터 파일을 동기화 하는거라 했는데.. 저널로그 끄면 파일 없는거 아닌가? 메모리에 있는 더티 페이지를 쓴다는 의미로 받아들이면 되는건가?_~~ (**Need to research**) 
 
     - 레플리카셋의 세컨더리에서 저널로그를 활성화 할 경우 빈번한 체크포인트가 실행되는거 대신 저널로그를 매우 빈번하게 디스크로 동기화 하도록 변경됐다
 
         → ~~_이거 무슨 말이지... 체크 포인트가 저널로그를 파일로 동기화 하는거라했는데... ? **  (p.89 아래)_~~ (**Need to research**)
         
         
> **캐시 이빅션**

    - WiredTiger 스토리지 엔진은 공유 캐시를 위해 지정된 크기의 메모리 공간만 사용해야 하는데, 이를 위해서는 항상 공유 캐시 내에 적재할 수 있는 공간을 항상 적절히 유지해야한다. WiredTiger 에서는 공유 캐시에 저절한 빈 공간을 유지하기 위해 이빅션 모듈을 가지며 이를 "이빅션 서버"라고도 표현한다. 백그라운드 스레드로서 실행되며 공유 캐시에 적재된 자주 사용되지 않는 페이지를 제거한다. 이빅션 Thread가 적절히 공유 캐시의 여유 공간을 확보하지 못하면 사용자 쿼리를 처리하는 포그라운드 Thread가 직접 캐시 이빅션을 실행한다. 이로 인해 MongoDB의 쿼리 성능은 현저하게 떨어지게 된다. 

> **운영체제 캐시(페이지 캐시)**


# 3. 복제
> **3-1. Primary**

    - 변경을 처리할 수 있는 유일한 맴버. MongoDB에서는 기본적인 데이터 조회 쿼리도 프라이머리로 요청

> **3-2. Secondary**

    - 프라이머리가 처리한 변경 데이터를 실시간으로 가져와 Primary와 동일한 데이터 셋 유지한다. 하나의 레플리카셋에서 Secondary는 1개이상 존재 가능. Primary가 응답 불가 상태가 되면 세컨드리 멤버중 하나가 프라이머리 맴버가 됨. Secondary가 Primary가 되는걸 프로모션이라 한다.
        - 데이터 변경 요청은 처리할 수 없지만 읽기 요청은 처리할 수 있기 때문에 읽기 쿼리의 부하를 분산하는 용도로 활용 할 수 있음
        - Read Preference 옵션으로 Secondary에서 읽기 요청은 할 수 있지만 특정 세컨드리를 지정할 순 없음
        - 고가용성과 읽기 쿼리 부하 분산을 용도로 활용될 수 있지만 백업 용도와는 거리가 멀다. 사용자의 실수 등으로 인해서 Primary에서 유실된 데이터는 Secondary에서도 동일하게 유실된다.
    
> **3-3. Avitor**    

    - 실제 사용자 데이터를 전혀 가지지 않고 프라이머리 멤버로부터 OpLog를 가져오지도 않는다. MongoDB에서는 특정 조건을 갖춰야 프라이머리 선출 가능하다. 정족수를 채우기 위한 추가 맴버
    - 선출
        - 프라이머리 멤버가 없으면 사용자의 데이터 변경 요청을 처리할 수 없게 되며 Read Preference 옵션에 따라서 때로는 읽기 쿼리조차도 불가능 할 수 있다. 그래서 레플리카 셋에서 프라이머리 맴버가 없어진 것을 알아채면 즉시 새로운 프라이머리를 선출한다

        - MongoDB 3.0 까지의 프라이머리 선출 방식은 Protocol Version 0이라고 하고 MongoDB 3.2 버전부터 도입된 방식을 Protocol Version 1이라고 한다

> **3-4. 프라이머리 텀**

    - 투표 식별자. 레플리카셋의 각 맴버들이 프라이머리 선출 시도할 때마다 1씩 증가하는 논리적인 시간 값. 단순 투표할 때만 사용되는 것이 아니라 프라이머리 맴버가 사용자의 데이터 변경 요청을 실행한 다음 변경 내용을 OpLog에 기록할 때마다 식별자를 기록. 그 정보를 기반으로 특정 OpLog가 어느 맴버가 프라이머리 였을 때의 로그인지 식별할 수 있게 해준다.

> **3-4. 프라이머리 스탭 다운**

    - 레플리카셋에서 프라이머리가 보이지 않으면 다른 세컨드리 멤버들은 모두 자신의 레켈리카 셋에 프라이머리 멤버가 없다고 판단하고 레플리카 셋 설정에 명시된 electionTimeoutMillis 내에 응이 없으면 레플리카 셋의 각 멤버는 프라이머리가 없어졌다고 판단하고 즉시 새로운 프라이머리를 선출하기 위한 투표를 시작한다

        - 관리자가 의도적으로 기존의 프라이머리를 세컨드리로 내리는 것도 가능하다
            - rs.stepDown(stepDownSecs, secondaryCatchUpPeroodSecs)
                - 현재 프라이머리인 맴버에서만 실행 가능
                - 명령을 요청받은 프라이머리는 stepDwonSecs 파라미터에 지정된 시간 동안 다시 프라이머리가 될 수 없다.
                - 프라이머리가 스탭다운 되는 시점에 다른 세컨드리가 기존 프라이머리의 OpLog에서 모든 변경사항을 가져왔다는 보장을 하기 어렵다. secondaryCatchUpPeroodSecs 파라미터의 시간 동안 새로운 프라이머리를 선출하지 않고 기다리면서 밀려있던 복제가 동기화를 기다린다. 단, 최대 secondaryCatchUpPeroodSecs 시간동안 기다린다는 의미이고 복제가 더 빨리 완료되면 새로운 프라이머리 선출을 시작한다
            - rs.reconfig()
                - 레플리케이션 역할을 변경하는 직접적인 명령어는 아니다.
                - 레플리카셋 맴버의 priority를 변경하면 기존의 프라이머리가 즉시 세컨드리로 전환된다
            
> **3-6. 선출 시나리오**

    - 세컨드리 맴버는 다른 맴버로 전송한 하트비트 메시지에 대해서 지정된 시간 동안 응답이 없다면 그 멤버가 응답 불능 상태라고 인지만 하게된다. 하지만 프라이머리가 응답이 없으면 세컨드리 맴버는 그 즉시 새로운 프라이머리를 선출해야 한다.

    - 만약 프라이머리 서버는 정상 동작하지만 멤버간의 네트워크 문제가 있을 때 세컨드리가 새로운 프라이머리가 되서 서로가 자기가 프라이머리라고 생각하는 경우가 발생할 수 있는데 이를 스플릿 브레인이라 한다. 이런 현상을 방지하기 위해 프라이머리는 전체의 과반수 멤버와 통신이 되지 않으면 자동으로 프라이머리에서 세컨드리로 강등된다.

    - 프라이머리 선출 과정은 Self Election 이다. 절대 다른 세컨드리 멤버를 프라이머리 후보로 추천하지 않는다. 프라이머리가 없어지면 자기 자신이 바로 프라이머리 선출 투표를 개시하게 되는데 후보는 반드시 자기 자신이다.

    - 프라이머리가 돼도 괜찮을지 판단하는 기준은 아래와 같다 (A가 B에게 동의를 구할 경우)
        - A가 현재 나(B)와 같은 레플리카 셋 소속 맴버인가?
        - A의 우선순위가 현재 레플리카 셋에 있는 모든 맴버의 우선순위와 같거나 더 큰 값을 가지고 있는가?
        - A가 요청한 투표의 텀이 내가 지금까지 참여했던 투표의 텀보다 큰 값인가?
        - A가 요청한 투표의 텀에 내가 투표한 적이 없는가?
        - A가 나보다 최신 데이터를 가지고 있거나 동등한 데이털르 가지고 있는가? (OpLog의  OpTime으로 판단)

        - 이중 하나라도 거짓이면 거부 * 단, protocol version 1부터는 거부를 행사할 필요 없이 새로운 투표를 시작함으로서 거부권이 행사됨과 동시에 새로운 투표를 시작한다
    
> **3-7. 정족수의 의미**

    - 레플리카셋의 각 맴버는 votes 옵션의 값으로 0 또는 1을 가질 수 있다. 0인 맴버는 정족수를 판단하는 기준에 포함되지 않는다.
    - 투표권을 가지고 있다고 해서 모든 맴버가 선출에 참여할 수 있는것은 아니다. 데이터 복제의 상태에 따라서 여러 상태를 가질 수 있는데 아래 상태의 투표권을 가진 맴버만 투표에 참여할 수 있다
        - PRIMARY
        - SECONDARY
        - RECOVERING
        - ARBITER
        - ROLLBACK

        - 투표에 참여한다는 의미는 무엇일까? ** → 내가 프라이머리가 되겠다? 아니면 누군가 나에게 동의를 구할때 찬성/거부 하는 걸 말하는건가? (**Need to research**)
        - 전자로 보인다. 단, 투표권이 없어도 동의 요청에 대해서 거부는 할 수 있다. 그렇다는건 투표권이 없는 맴버한테도 애초에 동의를 구하는건가? (**Need to research**)
    
> **3-8. 롤백**

    - 기존 프라이머리 맴버가 네트워크 장애로 연결 불가가 되면서 세컨드리로 강등 된 후 새로운 프라이머리가 선출되었을 때 기존 프라이머리가 가진 데이터 중 새로운 프라이머리보다 최신 OpLog를 삭제하는 과정. 이때 단순 OpLog 삭제만이 아닌 실제 컬렉션의 도큐먼트를 찾아서 같이 삭제하거나 변경 전 데이터로 돌리는 작업을 한다. 도큐먼트를 삭제한 경우 현재 OpLog만으로는 삭제된 도큐먼트의 값을 알 수 없기 때문에 롤백 대상 OpLog의 프라이머리 키를 기준으로 다른 리플리카 셋 멤버가 가진 최종 버전의 도큐먼트를 가져와 롤백을 수행한다.

        - 롤백은 최대 300M까지 가능하다. 새로운 프라이머리 동기화 지점을 찾지 못하고 에러 메시지를 출력하고 복구를 멈춘다.
    
> **3-9. 복제**

    - 복제로그 구조
        - 변경 요청은 프라이머리 맴버만 할 수 있으며 프라이머리 맴버는 변경 내용을 별도의 컬렉션에 저장한다. 레플리카셋의 모든 맴버는 프라이머리로부터 이 그 로그를 가져와 재생함으로써 프라이머리 데이터를 동기화한다.
        - 이 복제용 로그를 OpLog라 하는데 다른 DBMS와 달리 MongoDB는 이 로그를 데이터베이스 서버의 [oplog.rs](http://oplog.rs) 란 이름의 컬렉션으로 기록한다

    - OpLog
        - 디스크 사용되지 않은 공간이 5% ↔ 50GB이내로 기본 설정됨
        - MongoDB는 처음 시작하면 기본적으로 local이라는 이름의 데이터베이스를 생성하는데 local 데이터베이스는 [oplog.rs](http://oplog.rs)를 포함해서 몇개의 MongoDB서버 자신을 위한 컬렉션을 생성한다. local이라는 이름의 데이터베이스에 저장된 컬렉션의 변경 내용은 oplog.rs 에 기록되지 않는다

    - 레플리카셋
        - 하트비트 protocolVersion 1부터 electionTimeoutMillis 옵션의 값을 통해 장애 감지 시간을 설정 가능
        - 멤버 우선순위
        - 투표권
        - 레플리카셋은 최대 50개 참여 가능하고 투표권은 최대 7개의 맴버만 가질 수 있음
        - 히든맴버
        - 읽기쿼리 분산
            - 세컨더리로 동기화 지연이 허용되지 않는다면 WriteConcern ** 사용 → 이게 뭔지 설명 필요

> **3-10. 초기 동기화**

    - 수동
        - 다른 리플리케이션 맴버의 데이터를 복사해와서 초기 동기화를 수행하는 경우
            - 데이터파일 복사 시 레플리케이션 맴버들이 갖고 있는 가장 늦은 OpLog 이후의 데이터를 복사해야 한다. → 이유 추가로 설명 필요 **
    - 자동
        - MongoDB 서버가 다른 멤버로부터 자동으로 데이터 복사
            - 데이터베이스 복제  (primary index만 생성)
            - 데이터 동기화 → OpLog 용량 제한 설명 필요**
		    - 인덱스 생성

    - 실시간 복제 
        - 복제 아키텍쳐 그림 필요 **
        - 세컨드리 맴버의 읽기 일관성 → 글로벌 잠금 및 멀티스레드 설명 필요 **

# 4. 샤딩
> **4-1. 샤딩 종류**    

    - 수평
    - 수직

> **4-2. 필요성**

    - 사용량이 많아질때 서버의 사양을 높여서 처리 속도를 안정화하는 스케일 업을 할 수도 있지만 사양을 높이는 방식은 상당히 빨리 그 한계에 이르게 될 가능성이 높다

> **4-3. 컨피그 서버란?**

    - SCCC (미러링 방식)
        - 서로 관계 없는 컨피그 서버를 설치하고 응용 프로그램에서 3대의 컨피그 서버에 모두 접속하여 각 서버의 데이터를 동기화 하는 방식을 말함. 응용 프로그램은 MongoDB서버나 라우터를 말함.
        - 라우터 서버가 청크 정보를 변경하고자 할 때 3대의 컨피그 서버에 접속하여 청크 정보를 변경하는 문장을 각각 실행하고 모두 완료되면 커밋을 수행하는 분산 트랜젝션을 실행하는 방식으로 처리한다.
            - 3.4 버전부터 완전히 없어졌다.
    - CSRS (레플리카 셋)
        - 샤드 서버의 구성 방식과 같음
        - 모든 메타정보의 조회 및 변경 쿼리는 ReadConcern 과 WriteConcern을 majority로 설정한다
        - 컨피그 서버는 반드시 WiredTiger 스토리지 엔진을 사용
        - 레플리카 셋은 아비터를 가질 수 없음
        - 레플리카 셋은 지연된 멤버를 가질 수 없음
        - 최소 3개의 멤버로 구성
    
> **4-4. 라우터란?**
    
    - 사용자의 쿼리 요청을 샤드 서버로 전달하고 샤드 서버로부터 쿼리 결과를 모아서 사용자에게 반환하는     프록시 역할을 수행한다.
    - 사용자 쿼리를 전달해야 할 샤드 서버를 결정하고 해당 샤드로 쿼리 전송
    - 샤드 서버로부터 반환 결과를 조합하여 사용자에 결과 반환
    - 샤드간 청크 밸런싱 및 청크 스플릿 수행 (이제 안하는거 아닌가? 컨피그에서 할텐데..? **)
    - 샤드 서버가 가지지 말아야 할 데이터인지 판단하고 가지지말아야 할 데이터는 제거하는 작업도 수행한다.    
            - 마이그레이션 중
            - 마이그레이션에 실패
            - 각 샤드서버에서 데이터를 직접 저장
            - 정렬이 필요하지 않은 쿼리에 대해서는 라우터가 각 샤드로부터 결과를 라운드 로빈 방식으로 가져온 다음 사용자에게 결과를 반환
        - 정렬이 필요할 경우 샤드 서버들 중에서 프라이머리 샤드를 결정하고 orderby 옵션을 같이 쿼리에 전송 후 프라이머리에서 나머지 샤드로부터 쿼리의 결과를 전달받아서 정렬을 수행 후 최종 결과를 라우터로 반환한다 → 여기서 말하는 프라이머리가 샤딩되지 않은 컬렉션을 저장하기 위한 프라이머리랑 같은 의미인가? **
        - LIMIT 옵션만 있을 경우 샤드 서버에 LIMIT 옵션을 전달하고 샤드 서버로부터 받은 결과에 대해 다시 LIMIT 수정
        - SKIP 의 경우 라우터에서 SKIP 옵션 제거 후 샤드 서버로 쿼리 전달 후 받은 결과를 병합 후 SKIP을 적용하여 결과 반환
    - 라우터의 쿼리 분산
        - 사용자의 쿼리가 이 샤딩 기준 키 값에 대한 조건을 가지고 있느냐에 따라서 라우터가 쿼리를 요청해야 할 샤드 서버를 결정하게 된다
            - 타겟쿼리
                - 사용자의 쿼리를 특정 샤드로만 요청하는 경우
                - 소량의 도큐먼트를 아주 빈번하게 읽어가는 쿼리는 타겟 쿼리가 효율적
                - 샤드키가 여러개로 구성된 경우에도 샤드의 범위를 제한 가능하다
                    - 샤드키의 일부만으로 특정 샤드를 제한할 수 있는 건 선행 필드가 조건으로 주어질 때만 가능하다
                - 조회뿐만 아니라 UPDATE, DELETE 도 타겟 쿼리로 작동할 수 있다
            - 브로드캐스트 쿼리
                - 모든 샤드 서버로 요청하는 경우
                - 아주 많은 도큐먼트를 한번에 읽는 쿼리가 가끔 실행되는 경우 브로드캐스트 쿼리가 서버의 자원을 더 효율적으로 사용할 수 있음
                - 샤드 키를 쿼리 조건으로 가지지 않는 경우에는 사용자의 쿼리를 라우터가 모든 샤드로 요청하고 결과를 병합하여 사용자에게 반환한다
                - 다중 업데이트의 경우 샤드키 포함 여부와 상관 없이 항상 브로드캐스트 쿼리로 실행
                - UpdateMany, DeleteMany 의 경우 대상 검색 조건이 반드시 샤드키를 모두 포함하는 경우에만 타겟 쿼리로 실행할 수 있다. 복합 필드를 샤드 키로 가지는 컬렉션에서는 샤드 키를 구성하는 모든 필드가 조건으로 사용되어야만 타겟 쿼리로 실행 할 수 있다

> **4-5. 라우터 배포**   

    - 응용 프로그램 서버와 배포
        - 메뉴얼에서 권장하는 가장 일반적인 형태
        - 응용 프로그램 서버에서 실행중인 라우터는 로컬 서버에서 실행중인 응용 프로그램 서버로부터의 연결만 처리
            - MongoDB 드라이버가 사용하는 ConnectionString에는 로컬의 라우터로만 연결하도록 "127.0.0.1:2701"만 명시한다
        - 일반적으로 라우터는 많은 시스템 자원을 사용하지 않도록 설계 되었음 따라서 응용 프로그램 서버가 많은 CPU, MEMORY를 사용하여도 라우터가 미치는 영향은 미미
        - 응용 프로그램 서버와 라우터가 같은 물리 서버에 위치하므로 네트워크 레이턴시를 최소화 할 수 있다.
        - 응용 프로그램 서버의 수가 늘어날 수록 샤드서버 입장에서 커넥션 수가 늘어나게 된다.
        - MongoDB 라우터만 응답할 수 없게 될 때 응용 프로그램이 어떻게 처리해야 하는지에 대한 고민이 필요함. → 이건 다른 배포 형태도 다 똑같은거 아닌가? **
    - 전용 라우터 서버 배포
        - 몽고디비 라우터를 전용의 서버에서 실행하고 응용프로그램 서버는 하나 이상의 라우터 서버로 접속하여 쿼리를 실행
        - 응용 프로그램 서버 수만큼 라우터가 실행될 필요가 없으므로 몽고디비 라우터 수가 줄어들고 그로 인해 샤드 서버와 맺어야 하는 커넥션 수도 줄어듬
            - 몽고디비 드라이버 ConnectionString에 1개 이상의 라우터 주소를 명시해야한다
            - Java의 경우 2.X 버전은 가까운 라우터를 선택해서 사용, 3.X 버전은 랜덤하게 요청, 특정 라우터가 응답 못할 경우 블랙 리스트에 등록하고 제외하도록 구현되어있다
    - L4와 배포
        - 커서 문제 발생
        - 네트워크 왕복 시간을 길게 만듬
    - 샤드 서버에 배포
        - 각 샤드서버가 구동중인 라우터로 결과를 반환하고 라우터 서버가 클라이언트로 결과를 보내야하기 때문에 네트워크 사용량이 최대 2배가 될 수 있다
    - 컨피그 서버와 함께 배포
        - 라우터를 데이터밸런싱과 관련된 관리 작업을 위해서 사용할때는 무관  (밸런싱은 3.4부터 라우터가 한다)
    - 샤드 알고리즘
        - 레인지
            - 샤드 키의 값을 기준으로 범위를 나누고 사용자 데이터가 어디 저장될 지 결정한다.
            - 샤드 키 값이 어떠한 변형 과정을 거치지 않는다.
            - 가장 큰 장점은 범위 검색 쿼리를 타겟 쿼리로 실행한다.
            - 균형있게 데이터 분산되지 않을 가능성이 높다
                - *가능하면 해시 샤딩을 사용하고 사용할 수 없을 때 레인지  샤딩을 사용하라*
        - 해시
            - 샤드 키 값을 그대로 청크 할당에 사용하는 것이 아닌 해시 값을 이용해서 처으를 할당하는 샤딩 방식
            - 레인지 샤딩의 경우 비슷한 샤드키의 경우 특정 청크에 몰리는 경우가 있고 그에 따라 특정 샤드 서버에 부하가 높아질 수 있지만 해시 샤딩의 경우 매우 균등하게 분포된다
            - 범위 검색 쿼리는 브로드캐스트 쿼리로 실행된다.
                - 샤드키의 원본 값으로 청크를 결정하는 것이 아니라 해시 함수로 결정하므로 당연!
            - 샤드키 필드에 대해서 해시 인덱스를 생성해야 한다 → 이건 왜? **
                - 그냥 규칙인듯 하다. 그에 따라 해시 인덱스의 단점을 그대로 가질 수 밖에 없다
                    - 단일 필드에 대해서만 해시 생성 가능
                    - 멀티 키 필드에 대해서는 해시 인덱스 생성 불가
                    - 부동 소수점 필드는 소수점 이하를 버리고 해시 함수 수행
                    - 2^53 보다 큰 부동 소수점에 대해서는 해시 인덱스를 지원하지 않음
        - 지역 기반 샤딩
            - 해시 샤딩이나 레인지 샤딩 처럼 독립적으로 사용 불가능하고 레인지 샤딩이나 해시 샤딩과 반드시 함께 사용해야한다.
            - 모든 데이터를 커버하지 않고 선택적으로 적용할 수 있다. 관심 대상에 대해서만 샤딩 알고리즘을 적용할 수 있다.
            - 샤드별로 태그를 할당하고 샤드 키 범위별로 태그를 할당해야함 → 잘 이해안됨 ** ??
            - 라우터는 지역기반 샤딩에 대해 고려하지 않는다. 밸런서가 청크의 샤드 키 범위에 기반해서 태그를 알아내고 그 태그를 담당하는 샤드 서버를 찾아 그쪽 샤드 서버로 청크를 마이그레이션 한다
            - 해시 샤딩과 궁합이 안좋은 듯 하다 → 조금 더 조사해보겠다 **
    - 청크
        - 청크 밸런스
            - 샤드 서버는 가능한 균등한 수준의 데이터를 가지고 사용자의 요청을 처리하려고 노력함. 모든 샤드가 동등한 부하의 요청을 처리할 때 최고의 성능을 발휘할 수 있기 때문임. 샤드키나 사용자 데이터에 따라 불균형은 언제든지 발생할 수 있고 몽고디비는 이러한 샤드 간 데이터 불균형 현상을 피하기 위해 각 샤드가 가진 청크의 개수를 동일하게 만드려 노력함. 더불어 하나의 청크가 너무 커지면 청크 하나의 이동에 너무 많은 시스템 자원이 소모되므로 너무 비대해지는 것을 막기 위해 청크에 데이터가 저장될 때마다 청크를 스플릿 해야할지 체크한다.

            3.4 버전부터 밸런서 프로세서를 컨피그  서버에서 수행하도록 변경됐다. 컨피그 서버의 프라이머리 맴버만 분산을 담당하므로 경합할 필요성이 없어졌다. 이로인해 라우터는 오직 사용자 쿼리를 샤드로 전달하는 역할만 처리하게 되었다. 

            청크의 이동은 다중으로 실행될 수 있게 되었는데, 청크 이동에 참여하는 샤드 서버가 같다면 동시에 실행될 수 없다.  (즉, 서로 교차해서 이동하는 것은 다중으로 실행될 수 있다.) → 샤드01 에서 02, 04로 청크를 이동하는 작업은 동시에 실행될 수 없다. 

            - 샤드 클러스터 밸런서
                - 청크를 가장 많이 가진 샤드와 청크를 가장 적게 가진 샤드의 청크 개수가 임계치 이상 차이나면 청크 이동을 실행한다.
        - 청크 스플릿
            - 설정한 청크 사이즈보다 크거나 최대 도큐먼트보다 많이 가진 청크에 대해 자동 실행된다
            - 가끔 비대해진 청크를 스플릿하지 못할 수도 있고 대량의 데이터를 적재하기 전에 미리 청크를 잘게 쪼개서서 분산할 수 있다. 관리자에 의해 수동으로 실행된다
        - 청크 머지
            - 사용자 데이터가 변경하며 특정 청크가 빈청크가 되기도함. 또는 미리 청크를 스플릿했는데, 의도대로 데이터가 저장되지 않아 빈 청크일 수 있다. 이렇게 빈 청크의 경우 주위의 연속된 청크와 병합할 수 있다.
                - 병합하고자 하는 청크는 같은 샤드에 존재
                - 병합되는 청크 중 최소 하나는 빈 청크
                - 연속된 샤드키 범위의 청크 병합 가능
        - 청크 이동
            - 밸런서는 샤드의 청크 개수를 비교해 특정 샤드에 청크 개수가 상대적으로 많을 경우 샤드간 청크를 이동한다.
            - 실패 케이스
        - 청크 이동 실패
            - 청크 사이즈가 설정 값보다 크거나  일정 갯수 이상의 도큐먼트를 가지게 되어 점보 청크로 플래그가 활성화 되어 이동 실패
        - 점보 청크
            - 단순 도큐먼트가 많은 경우 스플릿 가능
            - 하나의 샤드키는 두개 이상의 청크에 포함될 수 없음. 샤드키 값이 동일한 도큐먼트들이 많이 저장되면 스플릿에 실패하여 점보 청크가 됨. 이 경우 근본적인 해결책은 샤드키를 변경하는 것임
    - 프라이머리 샤드
        - 샤드 클러스터에서 샤딩되지 않은 컬렉션을 저장하는 샤드를 의미
        - 샤딩되지 않은 컬렉션을 가지고 있을 경우 sh.removeShard 명령으로 샤딩되지 않은 컬렉션을 다른 샤드로 옮길 수 없다.
    - 샤딩의 제약
        - 트랜젝션
        - 유니크 인덱스
            - 프라이머리키의 중복 체크
        - 기존 컬렉션에 청크 적용
        
# 5. 인덱스

> - 저장 성능을 희생해서 읽기 속도를 향상 시키기 위해 존재    
> - 역할별로 구분하면 프라이머리인덱스/세컨드리인덱스    
> - 저장방식별로 구분하면 B-tree인덱스/해시인덱스    
> - 데이터중복 여부로 구분하면 유니크와 유니크하지 않은 인덱스    
> - 기능별로 구분하면 전문 검색용 인덱스와 공간 검색용 인덱스    

    - MongoDB의 프라이머리/세컨드리 인덱스는 로컬 인덱스로 관리되므로 샤드가 저장하고 있는 도큐먼트에 대해서만 인덱스를 가진다. 그래서 프라이머리 인덱스나 유니크인덱스는 샤드키를 반드시 포함해야한다.

> **5-1. 클러스터링 인덱스**

    - 3.6 까지 클러스터링 인덱스를 지원하지 않음
    - 인덱스의 키값 순서대로 데이터를 저장하는 구조라서 insert가 느리다. 검색을 수행하는 경우에는 별도의 랜덤 액세스 없이 레코드를 읽어서 매우 빠르게 레인지 스캔을 수행할 수 있다.

> **5-2. MMApv1의 Record Id**    

    - 실제 데이터 파일과 파일 내에서 도큐먼트의 위치로 구성
    - 도큐먼트가 데이터 파일의 다른 위치로 옮겨지면 프라이머리 키를 포함해서 모든 인덱스의 엔트리를 변경해야함
    - 위의 단점을 보완하기 위해 paddingFactor라는 개념이 디스크의 데이터 파일 용량을 늘리는 주범
    - 인덱스에서 특정 키를  찾기만하면 키의 엔트리가 가진 주소를 이용해 도큐먼트를 바로 찾을 수 있어 읽는 속도가 빠름
    
> **5-3.WiredTiger의 Record Id**        

        - 도큐먼트마다 고유의 식별자를 할당해서 Record-Id로 부여한다. 흔히 자동 증가값 방식을 사용한다
        - 도큐먼트 크기가 커져 데이터 파일 내에서 이동되더라도 처음 할당된 논리 주소 값은 변하지 않고 계속 유지
        - 자동 증가 값인 Record-Id로 데이터 파일의 도큐먼트를 찾아가기 위해 내부에 Record-Id 값을 인덱스 키로 가지는 내부 인덱스를 하나 더 가진다. 정확하게는 Record-Id 값을 키로 하는 클러스터링 인덱스를 가진다.
    - 로컬 인덱스
    - 복합 인덱스

# 6. 잠금과 트랜젝션

| | Intent Shared | Intent Exclusive | Shared | Exclusive |
| ------ | ------ | ------ | ------ | ------ |
| Intent Shared | O | O | O | X |
| Intent Exclusive | O | O | X | X |
| Shared | O | X | O | X |
| Exclusive | X | X | X | X |

&nbsp;    
    - 글로벌, 데이터베이스, 컬렉션 오브젝트의 경우 MongoDB 서버가 할당 및 해제, 도큐먼트의 경우만 WiredTiger 스토리지 엔진이 담당

    - 잠금 Yield
        - 설정된 조건보다 오랜 시간 실행되거나 많은 자원을 소모하는 경우에는 잠깐 쉬었다가 다시 처리를 재개
        - 기본 값은 128개의 도큐먼트, 읽기 10밀리초 이상 Yield 실행
        - 실행 시 잠금뿐 아니라 스냅샷까지 모두 해제 그에 따라 128 개 이상의 도큐먼트를 읽는 중 데이터가 변경될 때 일관성 문제가 발생할 수 있다

    - 트랜젝션
        - 쓰기 충돌
            - RDBM의 경우 두 세션이 동시에 하나의 레코드를 변경하고자 할 때 해당 레코드에 잠금을 먼저 획득한 세션이 잠금을 해제 할때까지 나머지 세션은 기다림. MongoDB의 경우 다른 커넥션에 의해서 잠금이 걸려있으면 즉시 업데이트를 취소하고 재시도한다.
            - 따라서 MongoDB서버는 하나의 도큐먼트에 변경이 집중되면 CPI사용량은 높아지지만 처리 성능은 떨어질 수 있다.
        - 격리 수준
            - READ-UNCOMMITTED
            - READ-COMMITED
            - SNAPSHOT (REPEATABLE READ)
            - WiredTiger 엔진에서는 위와 같이 지원하지만 MongoDB서버에서는 SNAPSHOT레벨로 고정하여 초기화

    - Read & Write Concern
        - 필요성
            - MongoDB는 기본 분산 아키텍쳐를 선택하고 있기 때문에 레플리카 셋을 구성하는 맴버들 간의 동기화 제어가 필요함
        - Write Concern
            - UNACKNOWLEDGED
                - 실제 데이터 변경 적용 확인하지 않음
            - ACKNOWLEDGED
                - 메모리상에만 적용하고 응답
            - JOURNALED
                - 저널로그 기록하고 응답
            - FSYNC
                - 데이터 파일까지 동기화하고 응답
            - 레플리카셋의 여러 노드에 대해서 데이터 동기화가 필요할 경우 사용한다.
                - ACKNOWLEDGED + { w : "노드 수"}
                - 노드 수 대신 majority를 사용할 경우 과반수를 의미함

        - Read Concern
            - local
                - 쿼리가 실행되는 서버가 가진 최신의 데이터를 반환
            - majority
                - 레플리카 셋에서 다수의 맴버들이 최신의 데이터를 가졌을 때만 반환
            - linearizable
                - 레플리카셋의 모든 맴버가 가진 변경 사항에 대해서 반환
                - 프라이머리 맴버에서 쿼리를 실행할 때에만 사용할 수 있음
            - 롤백 될 가능성이 있는 데이터를 클라이언트로 보내지 않기위해 설정

        - 데이터의 변경이 롤백으로 손상되지 않을 정도 보장이 됐을 때 클라이언트에 결과를 반환하고 롤백되지 않을 데이터만 반환하도록 하려면 Read, Write Concern 을 모두 majority 로 설정해야한다.
            - 둘다 w : 2로 설정하면 왜 안되는거지? **

    - Read Preference
        - INSERT, UPDATE, DELETE에서는 적용 안됨
        - primary
            - "Default 설정", 프라이머리로만 요청하고 프라이머리가 없을 경우 쿼리 실행 실패
        - primaryPreferred
            - 가능하면 프라이머리로 쿼리 전송. 프라이머리 없을 경우에만 세컨더리로 전송
        - secondary
            - 세컨드리 맴버로만 전송
        - secondaryPreferred
            - 세컨드리 맴버로 전송을 기본으로 하고 세컨드리가 없을 경우 프라이머리로 전송
        - nearest
            - 몽고디비 드라이버나 라우터에서 수집하고 있는 응답 시간을 기준으로 가장 빠른 응답을 줄 수 있는 맴버로 요청

        - 세컨드리는 OpLog를 적용 시 글로벌 잠금을 거니 조회 쿼리를 실행 못함. 세컨드리 노드의 경우 사용자 퀄로 인해서 복제 지연이 발생할 수 있음

    - 샤딩 환경의 중복 도큐먼트
        - 고아 도큐먼트